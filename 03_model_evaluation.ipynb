{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook will evaluate the following models:\n",
    "\n",
    "- StarEncoder\n",
    "- CodeBert\n",
    "- CodeGen\n",
    "- FLAN-T5\n",
    "- CodeTrans\n",
    "\n",
    "The architecture, dataset, and training approaches of each model are compared in [model_comparisons.md](model_comparisons.md).\n",
    "\n",
    "Metrics will also be generated for each model:\n",
    "\n",
    "- Perplexity (MLM, CLM)\n",
    "- Accuracy, F1 Score, Precision, Recall (Text Classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    IntervalStrategy,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "import constants\n",
    "from utils import tokenize_dataset_example, prepare_starncoder_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # Sequences per batch\n",
    "EVAL_STEPS = 20  # Number of batches for evaluation\n",
    "EVAL_SIZE = EVAL_STEPS * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "\n",
    "raw_datasets = load_dataset(\"csv\", data_files=\"./data/commits.csv\").shuffle(seed=420)\n",
    "\n",
    "\n",
    "def tokenize_function(tokenizer: PreTrainedTokenizer, text_column: str = \"commit_msg\"):\n",
    "    def apply(example: dict):\n",
    "        result = tokenizer(example[text_column])\n",
    "        return result\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def concatenate_texts(max_input_length: int):\n",
    "    def apply(examples: dict):\n",
    "        concatenated_texts = {k: sum(examples[k], []) for k, v in examples.items()}\n",
    "        total_length = len(concatenated_texts[\"input_ids\"])\n",
    "        # Remove excess texts\n",
    "        cut_length = (total_length // max_input_length) * max_input_length\n",
    "        # Split texts from cut_length based on max_input_length\n",
    "        result = {\n",
    "            k: [\n",
    "                t[i : i + max_input_length]\n",
    "                for i in range(0, cut_length, max_input_length)\n",
    "            ]\n",
    "            for k, t in concatenated_texts.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_CHECKPOINT = \"bigcode/starencoder\"\n",
    "MODEL_CHECKPOINT = \"bigcode/starencoder\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 128  # max 1024 - higher value requires more VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Model\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Prepare Tokenizer\n",
    "tokenizer = prepare_starncoder_tokenizer(TOKENIZER_CHECKPOINT)\n",
    "\n",
    "# Prepare Datasets\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function(tokenizer, text_column=\"commit_msg\"),\n",
    "    batched=True,\n",
    "    remove_columns=[\"commit_msg\", \"remote_url\", \"date\", \"sha\", \"labels\"],\n",
    ")\n",
    "concatenated_datasets = tokenized_datasets.map(\n",
    "    concatenate_texts(MAX_INPUT_LENGTH), batched=True\n",
    ")\n",
    "\n",
    "# Prepare Evaluator\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "eval_dataset = concatenated_datasets[\"train\"]\n",
    "if EVAL_SIZE:\n",
    "    eval_dataset = eval_dataset.select(range(EVAL_SIZE))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/eval\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation\n",
    "\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 12.10\n"
     ]
    }
   ],
   "source": [
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeBert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_CHECKPOINT = \"microsoft/codebert-base\"\n",
    "MODEL_CHECKPOINT = \"microsoft/codebert-base\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 128  # max 512 - higher value requires more VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Model\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Prepare Tokenizer\n",
    "tokenizer = prepare_starncoder_tokenizer(TOKENIZER_CHECKPOINT)\n",
    "\n",
    "# Prepare Datasets\n",
    "raw_datasets = load_dataset(\"csv\", data_files=\"./data/commits.csv\")\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function(tokenizer, text_column=\"commit_msg\"),\n",
    "    batched=True,\n",
    "    remove_columns=[\"commit_msg\", \"remote_url\", \"date\", \"sha\", \"labels\"],\n",
    ")\n",
    "concatenated_datasets = tokenized_datasets.map(\n",
    "    concatenate_texts(MAX_INPUT_LENGTH), batched=True\n",
    ")\n",
    "\n",
    "# Prepare Evaluator\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "eval_dataset = concatenated_datasets[\"train\"]\n",
    "if EVAL_SIZE:\n",
    "    eval_dataset = eval_dataset.select(range(EVAL_SIZE))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/eval\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation\n",
    "\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 35125948.32\n"
     ]
    }
   ],
   "source": [
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_CHECKPOINT = \"Salesforce/codegen-350M-multi\"\n",
    "MODEL_CHECKPOINT = \"Salesforce/codegen-350M-multi\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 64  # max 2048 - higher value requires more VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Prepare Tokenizer\n",
    "tokenizer = prepare_starncoder_tokenizer(TOKENIZER_CHECKPOINT)\n",
    "\n",
    "# Prepare Datasets\n",
    "raw_datasets = load_dataset(\"csv\", data_files=\"./data/commits.csv\")\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function(tokenizer, text_column=\"commit_msg\"),\n",
    "    batched=True,\n",
    "    remove_columns=[\"commit_msg\", \"remote_url\", \"date\", \"sha\", \"labels\"],\n",
    ")\n",
    "concatenated_datasets = tokenized_datasets.map(\n",
    "    concatenate_texts(MAX_INPUT_LENGTH), batched=True\n",
    ")\n",
    "\n",
    "# Prepare Evaluator\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "eval_dataset = concatenated_datasets[\"train\"]\n",
    "if EVAL_SIZE:\n",
    "    eval_dataset = eval_dataset.select(range(EVAL_SIZE))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/eval\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True # can comment out if enough VRAM\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation\n",
    "\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 213.49\n"
     ]
    }
   ],
   "source": [
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAN-T5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
