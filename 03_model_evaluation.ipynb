{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook will be evaluating the following models:\n",
    "\n",
    "- FLAN-T5\n",
    "- CodeGen\n",
    "- CodeTrans\n",
    "- CodeBert\n",
    "- StarEncoder\n",
    "\n",
    "The architecture, dataset, and training approaches of each model will be\n",
    "discussed. Metrics will also be generated for each model.\n",
    "\n",
    "## Criterias\n",
    "\n",
    "- Trained on C/C++\n",
    "- Trained on Natural Language\n",
    "  - Prefably also with Git commits\n",
    "- Architecture\n",
    "  - Encoder (preferred)\n",
    "  - Decoder\n",
    "- Learning Objective\n",
    "  - Either Masked Language Modelling (MLM) or Casual Language Modelling (CLM)\n",
    "  - Both can be fine-tuned for text classification\n",
    "\n",
    "## Metrics\n",
    "\n",
    "- MLM\n",
    "  - Perplexity\n",
    "- Text Classification\n",
    "  - Accuracy\n",
    "  - F1 Score\n",
    "  - Precision\n",
    "  - Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    IntervalStrategy,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "import constants\n",
    "from utils import tokenize_dataset_example, prepare_starncoder_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # Sequences per batch\n",
    "EVAL_STEPS = 100  # Batch evaluation count\n",
    "EVAL_SIZE = EVAL_STEPS * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "\n",
    "raw_datasets = load_dataset(\"csv\", data_files=\"./data/commits.csv\").shuffle(seed=420)\n",
    "\n",
    "\n",
    "def tokenize_function(tokenizer: PreTrainedTokenizer, text_column: str = \"commit_msg\"):\n",
    "    def apply(example: dict):\n",
    "        result = tokenizer(example[text_column])\n",
    "        return result\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def concatenate_texts(max_input_length: int):\n",
    "    def apply(examples: dict):\n",
    "        concatenated_texts = {k: sum(examples[k], []) for k, v in examples.items()}\n",
    "        total_length = len(concatenated_texts[\"input_ids\"])\n",
    "        # Remove excess texts\n",
    "        cut_length = (total_length // max_input_length) * max_input_length\n",
    "        # Split texts from cut_length based on max_input_length\n",
    "        result = {\n",
    "            k: [\n",
    "                t[i : i + max_input_length]\n",
    "                for i in range(0, cut_length, max_input_length)\n",
    "            ]\n",
    "            for k, t in concatenated_texts.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAN-T5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGen\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2203.13474.pdf)\n",
    "- [GitHub](https://github.com/salesforce/CodeGen)\n",
    "- [HuggingFace](https://huggingface.co/docs/transformers/model_doc/codegen)\n",
    "\n",
    "### Overview\n",
    "\n",
    "- Released 2022\n",
    "- Architecure\n",
    "  - Decoder, Autoregressive\n",
    "- Learning Objective\n",
    "  - Next-token Prediction (CLM)\n",
    "- 3 Dataset Stages:\n",
    "  1. CodeGen-NL\n",
    "     - Dataset: The Pile\n",
    "     - Natural Language `1159.04 GB`, `354.7B Tokens`\n",
    "     - Code `95.16 GB`, `31.6B Tokens`\n",
    "  2. CodeGen-Multi\n",
    "     - Dataset: Google BigQuery\n",
    "     - Code `340 GB`, `119.3B Tokens`\n",
    "       - C/C++ `119 GB`, `19.B Tokens`\n",
    "  3. CodeGen-Mono\n",
    "     - Dataset: BigPython\n",
    "     - Code consists of Python, not necessary for our use case\n",
    "- 4 Checkpoints per variant\n",
    "  - 350M, 2.7B, 6.1B, 16.1B\n",
    "\n",
    "#### Pros\n",
    "\n",
    "- Trained on a lot of C/C++\n",
    "- Available checkpoints for small model\n",
    "  - 350M, 2.7B\n",
    "  - Can run on consumer GPUs\n",
    "\n",
    "#### Cons\n",
    "\n",
    "- Architecture not as ideal for text classification fine-tuning\n",
    "- Learning objective is CLM, not MLM\n",
    "  - However, there is a newer version `CodeGen2`, 2023\n",
    "    - Adds MLM training objective\n",
    "    - Encoder-Decoder architecture\n",
    "    - Uses more languages and more data, including C and C++, using `The Stack` dataset\n",
    "- Not explicitly trained on Git commits\n",
    "  - However, it may have slightly learned from the commit messages in `The Pile` dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeTrans\n",
    "\n",
    "- [Paper](https://arxiv.org/abs/2104.02443)\n",
    "- [GitHub](https://github.com/agemagician/CodeTrans)\n",
    "- [HuggingFace](https://huggingface.co/models?search=code_trans)\n",
    "- [Datasets](https://www.dropbox.com/sh/mzxa2dq30gnot29/AABIf7wPxH5Oe0PZHJ5jPV22a?dl=0)\n",
    "\n",
    "### Overview\n",
    "\n",
    "- Released 2023\n",
    "- Architecture\n",
    "  - Encoder-decoder (based on T5)\n",
    "- Learning Objective\n",
    "  - Text-to-text\n",
    "- Variants\n",
    "  - Function Documentation Generation (Python, Java, Go, Php, Ruby, JavaScript)\n",
    "    - [CodeSearchNet Corpus Collection](https://github.com/github/CodeSearchNet) Dataset\n",
    "  - Source Code Summarization (Python, SQL, C#)\n",
    "    - [CODENN StackOverflow](https://github.com/sriniiyer/codenn) Dataset\n",
    "  - Code Comment Generation (Java only)\n",
    "    - [DeepCom](https://github.com/xing-hu/DeepCom) Dataset\n",
    "  - Commit Message Generation (Java only)\n",
    "    - [CommitGen](https://sjiang1.github.io/commitgen) Dataset\n",
    "  - API Sequence Recommendation (Java only)\n",
    "    - [Deep API Learning](https://github.com/guxd/deepAPI) Dataset\n",
    "  - Programming Language and Synthesis (LISP only)\n",
    "    - [AlgoLisp](https://github.com/nearai/program_synthesis/tree/master/program_synthesis/algolisp) Dataset\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Multiple Variants Trained to Perform Specific Tasks\n",
    "- Source Code Summarization Model Trained on C#\n",
    "- Transformer-based Encoder-Decoder Model\n",
    "  - Good for Sequence-to-Sequence Tasks (e.g. Summarization, Translation)\n",
    "- Uses and Evaluates Different Training Stratagies\n",
    "  - Single-Task Learning, Transfer Learning, Multi-Task Learning, Multi-Task Learning with Fine-Tuning\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Not Trained on C dataset**\n",
    "- **Not Trained to Perform Masked Language Modeling**\n",
    "  - Needed for Accurate Text Classification\n",
    "- No Longer State-of-the-Art\n",
    "- **Model is Largely Abandoned**\n",
    "  - Incomplete README, last Git commit on 1 June 2021\n",
    "- Requires more work to prepare data: text-to-text format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeBert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarEncoder\n",
    "\n",
    "- [Blog](https://huggingface.co/blog/starcoder)\n",
    "- [Paper](https://arxiv.org/pdf/2305.06161.pdf)\n",
    "- [GitHub](https://github.com/bigcode-project/bigcode-encoder)\n",
    "- [HuggingFace](https://huggingface.co/bigcode/starencoder)\n",
    "- [Dataset](https://huggingface.co/datasets/bigcode/starcoderdata)\n",
    "- [Dataset Search](https://huggingface.co/spaces/bigcode/search)\n",
    "- [Dataset Portrait](https://stack.dataportraits.org/)\n",
    "\n",
    "### Overview\n",
    "\n",
    "- Released 2023\n",
    "- Architecture\n",
    "  - Encoder, Bi-directional (from Bert)\n",
    "- Learning Objective\n",
    "  - MLM\n",
    "  - Next Sentence Prediction (NSP)\n",
    "- Dataset (The Stack, Google BigQuery)\n",
    "  - Natural Language\n",
    "    - GitHub issues `54 GB`\n",
    "    - Git commits `64 GB`\n",
    "  - Code: Over 80 Languages\n",
    "    - C/C++ `103 GB`\n",
    "- 1 Checkpoint\n",
    "  - 125M\n",
    "\n",
    "#### Pros\n",
    "\n",
    "- Trained on a lot of C/C++\n",
    "- Trained on a lot of Git commits and GitHub issues\n",
    "  - Includes linux, httpd, and openssh-portable repositories\n",
    "- Is Encoder Architecture\n",
    "- MLM Learning Objective\n",
    "- Checkpoint is small enough to run on consumer GPUs\n",
    "\n",
    "#### Cons\n",
    "\n",
    "- Some model configuration required to only use the MLM objective\n",
    "  - Uses the training input: `[CLS]{Snippet-1}[SEP]{Snippet-2}[SEP]`\n",
    "  - Solutions:\n",
    "    - Fine-tune the model\n",
    "    - Or, proceed straight to text-classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_CHECKPOINT = \"bigcode/starencoder\"\n",
    "MODEL_CHECKPOINT = \"bigcode/starencoder\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 128  # max 1024 - higher value requires more vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Model\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Prepare Tokenizer\n",
    "tokenizer = prepare_starncoder_tokenizer(TOKENIZER_CHECKPOINT)\n",
    "\n",
    "# Prepare Datasets\n",
    "raw_datasets = load_dataset(\"csv\", data_files=\"./data/commits.csv\")\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function(tokenizer, text_column=\"commit_msg\"),\n",
    "    batched=True,\n",
    "    remove_columns=[\"commit_msg\", \"remote_url\", \"date\", \"sha\", \"labels\"],\n",
    ")\n",
    "concatenated_datasets = tokenized_datasets.map(\n",
    "    concatenate_texts(MAX_INPUT_LENGTH), batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "eval_dataset = concatenated_datasets[\"train\"]\n",
    "if EVAL_SIZE:\n",
    "    eval_dataset = eval_dataset.select(range(EVAL_SIZE))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/eval\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b145d87d50494791a207822e40c912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 7.69\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
